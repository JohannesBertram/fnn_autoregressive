{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from different FNN layers\n",
    "\n",
    "Sampling from the output is not yet integrated here, use sampling-fnn-outputs.ipynb for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "Using device: cuda\n",
      "Total number of parameters: 1261953\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import createFlowDataset, subps  # Assuming these functions are in your utils module\n",
    "from glob import glob\n",
    "from time import time\n",
    "import sys\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "print(torch.__version__)  # E.g., '1.10.0'\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "import fnn\n",
    "from fnn import microns\n",
    "from numpy import full, concatenate\n",
    "from fnn.microns.build import frame_autoregressive_model\n",
    "\n",
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Finds the most recent checkpoint file based on epoch number.\"\"\"\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"autoregressive_fnn_epoch_\") and f.endswith(\".pt\")]\n",
    "    if not checkpoints:\n",
    "        raise FileNotFoundError(f\"No checkpoint files found in {checkpoint_dir}\")\n",
    "    \n",
    "    # Extract epoch number and sort\n",
    "    epochs = [int(f.split('_')[-1].replace('.pt', '')) for f in checkpoints]\n",
    "    latest_epoch = max(epochs)\n",
    "    latest_epoch = 10\n",
    "    latest_file = os.path.join(checkpoint_dir, f\"autoregressive_fnn_epoch_{latest_epoch}.pt\")\n",
    "    return latest_file, latest_epoch\n",
    "\n",
    "checkpoint_dir = \"example_checkpoints\"\n",
    "pred_steps = 5\n",
    "latest_checkpoint_path, latest_epoch = get_latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "model = frame_autoregressive_model(pred_steps=pred_steps).to(device)\n",
    "model.load_state_dict(torch.load(latest_checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "################# SET PARAMS ##########################\n",
    "block = ['hidden'] # choose from 'inputs.0', 'inputs.1', 'inputs.2', 'blocks.0', 'blocks.1', 'blocks.2', 'hidden', 'recurrent.out', 'position', 'readout'\n",
    "n_fmaps_to_sample = 40\n",
    "samples_per_fmap = 50\n",
    "seed = 1\n",
    "\n",
    "################## MORE PARAMS ########################\n",
    "# I suggest leaving these unchanged for comparability\n",
    "\n",
    "LAYER_TYPE = 'act'\n",
    "MAX_SIDE = 32\n",
    "\n",
    "# Flow stimuli parameters\n",
    "scl_factor = 0.7\n",
    "N_INSTANCES = 3\n",
    "trial_len = 75 // 2  # Number of frames\n",
    "stride = 1\n",
    "\n",
    "model_name = 'auto1'\n",
    "\n",
    "## SAMPLING\n",
    "fmap_samp_method = 'maxFr'\n",
    "samp_max_one_dir = False # samples high activities to horizontal movement to the right only if set to True\n",
    "neur_samp_method = 'maxNr'\n",
    "\n",
    "input_shape = (144, 256)\n",
    "save_hidden = False # hacky hidden state debugging\n",
    "\n",
    "get_pos = True if \"position\" in block else False\n",
    "if get_pos:\n",
    "    block[block == \"position\"] = 'recurrent.out'\n",
    "\n",
    "get_act = any(item in block for item in ['inputs.0', 'inputs.1', 'inputs.2', 'blocks.0', 'blocks.1', 'blocks.2'])\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module name: , type: AutoregressiveVideoModel\n",
      "Module name: core, type: FeedforwardRecurrent\n",
      "Module name: core.feedforward, type: InputDense\n",
      "Module name: core.feedforward.inputs, type: ModuleList\n",
      "Module name: core.feedforward.inputs.0, type: Conv\n",
      "Module name: core.feedforward.inputs.0.weights, type: ParameterList\n",
      "Module name: core.feedforward.inputs.0.biases, type: ParameterList\n",
      "Module name: core.feedforward.inputs.1, type: Conv\n",
      "Module name: core.feedforward.inputs.1.weights, type: ParameterList\n",
      "Module name: core.feedforward.inputs.1.gains, type: ParameterList\n",
      "Module name: core.feedforward.inputs.1.biases, type: ParameterList\n",
      "Module name: core.feedforward.inputs.2, type: Conv\n",
      "Module name: core.feedforward.inputs.2.weights, type: ParameterList\n",
      "Module name: core.feedforward.inputs.2.gains, type: ParameterList\n",
      "Module name: core.feedforward.inputs.2.biases, type: ParameterList\n",
      "Module name: core.feedforward.blocks, type: ModuleList\n",
      "Module name: core.feedforward.blocks.0, type: Block\n",
      "Module name: core.feedforward.blocks.0.nonlinear, type: GELU\n",
      "Module name: core.feedforward.blocks.0.drops, type: ModuleList\n",
      "Module name: core.feedforward.blocks.0.drops.0, type: Dropout\n",
      "Module name: core.feedforward.blocks.0.drops.1, type: Dropout\n",
      "Module name: core.feedforward.blocks.0.convs, type: ModuleList\n",
      "Module name: core.feedforward.blocks.0.convs.0, type: Conv\n",
      "Module name: core.feedforward.blocks.0.convs.0.weights, type: ParameterList\n",
      "Module name: core.feedforward.blocks.0.convs.0.gains, type: ParameterList\n",
      "Module name: core.feedforward.blocks.0.convs.0.biases, type: ParameterList\n",
      "Module name: core.feedforward.blocks.0.convs.1, type: Conv\n",
      "Module name: core.feedforward.blocks.0.convs.1.weights, type: ParameterList\n",
      "Module name: core.feedforward.blocks.0.convs.1.gains, type: ParameterList\n",
      "Module name: core.feedforward.blocks.0.convs.1.biases, type: ParameterList\n",
      "Module name: core.feedforward.blocks.0.skips, type: ModuleList\n",
      "Module name: core.feedforward.blocks.0.skips.1, type: Conv\n",
      "Module name: core.feedforward.blocks.0.skips.1.weights, type: ParameterList\n",
      "Module name: core.feedforward.blocks.0.skips.1.gains, type: ParameterList\n",
      "Module name: core.feedforward.blocks.1, type: Block\n",
      "Module name: core.feedforward.blocks.1.nonlinear, type: GELU\n",
      "Module name: core.feedforward.blocks.1.drops, type: ModuleList\n",
      "Module name: core.feedforward.blocks.1.drops.0, type: Dropout\n",
      "Module name: core.feedforward.blocks.1.drops.1, type: Dropout\n",
      "Module name: core.feedforward.blocks.1.convs, type: ModuleList\n",
      "Module name: core.feedforward.blocks.1.convs.0, type: Conv\n",
      "Module name: core.feedforward.blocks.1.convs.0.weights, type: ParameterList\n",
      "Module name: core.feedforward.blocks.1.convs.0.gains, type: ParameterList\n",
      "Module name: core.feedforward.blocks.1.convs.0.biases, type: ParameterList\n",
      "Module name: core.feedforward.blocks.1.convs.1, type: Conv\n",
      "Module name: core.feedforward.blocks.1.convs.1.weights, type: ParameterList\n",
      "Module name: core.feedforward.blocks.1.convs.1.gains, type: ParameterList\n",
      "Module name: core.feedforward.blocks.1.convs.1.biases, type: ParameterList\n",
      "Module name: core.feedforward.blocks.1.skips, type: ModuleList\n",
      "Module name: core.feedforward.blocks.1.skips.1, type: Conv\n",
      "Module name: core.feedforward.blocks.1.skips.1.weights, type: ParameterList\n",
      "Module name: core.feedforward.blocks.1.skips.1.gains, type: ParameterList\n",
      "Module name: core.feedforward.blocks.2, type: Block\n",
      "Module name: core.feedforward.blocks.2.nonlinear, type: GELU\n",
      "Module name: core.feedforward.blocks.2.drops, type: ModuleList\n",
      "Module name: core.feedforward.blocks.2.drops.0, type: Dropout\n",
      "Module name: core.feedforward.blocks.2.drops.1, type: Dropout\n",
      "Module name: core.feedforward.blocks.2.convs, type: ModuleList\n",
      "Module name: core.feedforward.blocks.2.convs.0, type: Conv\n",
      "Module name: core.feedforward.blocks.2.convs.0.weights, type: ParameterList\n",
      "Module name: core.feedforward.blocks.2.convs.0.gains, type: ParameterList\n",
      "Module name: core.feedforward.blocks.2.convs.0.biases, type: ParameterList\n",
      "Module name: core.feedforward.blocks.2.convs.1, type: Conv\n",
      "Module name: core.feedforward.blocks.2.convs.1.weights, type: ParameterList\n",
      "Module name: core.feedforward.blocks.2.convs.1.gains, type: ParameterList\n",
      "Module name: core.feedforward.blocks.2.convs.1.biases, type: ParameterList\n",
      "Module name: core.feedforward.blocks.2.skips, type: ModuleList\n",
      "Module name: core.feedforward.blocks.2.skips.1, type: Conv\n",
      "Module name: core.feedforward.blocks.2.skips.1.weights, type: ParameterList\n",
      "Module name: core.feedforward.blocks.2.skips.1.gains, type: ParameterList\n",
      "Module name: core.feedforward.out, type: Conv\n",
      "Module name: core.feedforward.out.weights, type: ParameterList\n",
      "Module name: core.feedforward.out.gains, type: ParameterList\n",
      "Module name: core.feedforward.out.biases, type: ParameterList\n",
      "Module name: core.recurrent, type: CvtLstm\n",
      "Module name: core.recurrent.drop_x, type: Dropout\n",
      "Module name: core.recurrent.drop_h, type: Dropout\n",
      "Module name: core.recurrent.conv, type: Conv\n",
      "Module name: core.recurrent.conv.weights, type: ParameterList\n",
      "Module name: core.recurrent.scales, type: ParameterList\n",
      "Module name: core.recurrent.proj_x, type: Accumulate\n",
      "Module name: core.recurrent.proj_x.0, type: InterGroup\n",
      "Module name: core.recurrent.proj_x.0.weights, type: ParameterList\n",
      "Module name: core.recurrent.proj_x.0.gains, type: ParameterList\n",
      "Module name: core.recurrent.proj_x.1, type: Conv\n",
      "Module name: core.recurrent.proj_x.1.weights, type: ParameterList\n",
      "Module name: core.recurrent.proj_x.1.gains, type: ParameterList\n",
      "Module name: core.recurrent.proj_x.1.biases, type: ParameterList\n",
      "Module name: core.recurrent.proj_x.2, type: Conv\n",
      "Module name: core.recurrent.proj_x.2.weights, type: ParameterList\n",
      "Module name: core.recurrent.proj_x.2.gains, type: ParameterList\n",
      "Module name: core.recurrent.proj_q, type: Conv\n",
      "Module name: core.recurrent.proj_q.weights, type: ParameterList\n",
      "Module name: core.recurrent.proj_k, type: Conv\n",
      "Module name: core.recurrent.proj_k.weights, type: ParameterList\n",
      "Module name: core.recurrent.proj_v, type: Conv\n",
      "Module name: core.recurrent.proj_v.weights, type: ParameterList\n",
      "Module name: core.recurrent.proj_i, type: Conv\n",
      "Module name: core.recurrent.proj_i.weights, type: ParameterList\n",
      "Module name: core.recurrent.proj_i.gains, type: ParameterList\n",
      "Module name: core.recurrent.proj_i.biases, type: ParameterList\n",
      "Module name: core.recurrent.proj_f, type: Conv\n",
      "Module name: core.recurrent.proj_f.weights, type: ParameterList\n",
      "Module name: core.recurrent.proj_f.gains, type: ParameterList\n",
      "Module name: core.recurrent.proj_f.biases, type: ParameterList\n",
      "Module name: core.recurrent.proj_g, type: Conv\n",
      "Module name: core.recurrent.proj_g.weights, type: ParameterList\n",
      "Module name: core.recurrent.proj_g.gains, type: ParameterList\n",
      "Module name: core.recurrent.proj_g.biases, type: ParameterList\n",
      "Module name: core.recurrent.proj_o, type: Conv\n",
      "Module name: core.recurrent.proj_o.weights, type: ParameterList\n",
      "Module name: core.recurrent.proj_o.gains, type: ParameterList\n",
      "Module name: core.recurrent.proj_o.biases, type: ParameterList\n",
      "Module name: core.recurrent.out, type: Conv\n",
      "Module name: core.recurrent.out.weights, type: ParameterList\n",
      "Module name: core.recurrent.out.gains, type: ParameterList\n",
      "Module name: core.recurrent.out.biases, type: ParameterList\n",
      "Module name: decoder, type: FrameDecoder\n",
      "Module name: decoder.net, type: Sequential\n",
      "Module name: decoder.net.0, type: Upsample\n",
      "Module name: decoder.net.1, type: Conv2d\n",
      "Module name: decoder.net.2, type: GELU\n",
      "Module name: decoder.net.3, type: Upsample\n",
      "Module name: decoder.net.4, type: Conv2d\n",
      "Module name: decoder.net.5, type: GELU\n",
      "Module name: decoder.net.6, type: Upsample\n",
      "Module name: decoder.net.7, type: Conv2d\n",
      "Module name: decoder.net.8, type: GELU\n",
      "Module name: decoder.net.9, type: Conv2d\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(f\"Module name: {name}, type: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_sample(batch_size=1):\n",
    "\n",
    "    units = 9941\n",
    "\n",
    "    mu = Parameter(torch.zeros(units, 2))\n",
    "    mu.scale = units\n",
    "    mu.decay = False\n",
    "\n",
    "    sigma = Parameter(torch.eye(2).repeat(units, 1, 1))\n",
    "    sigma.scale = units\n",
    "    sigma.decay = False\n",
    "\n",
    "\n",
    "    x = mu.repeat(batch_size, 1, 1)\n",
    "    x = x + torch.einsum(\"U C D , N U D -> N U C\", sigma, torch.randn_like(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "def process_core_output(core):\n",
    "    #position = torch.load(\"test_pos.pt\")\n",
    "    #self.position.mean.expand(core.size(0), -1, -1)\n",
    "    #print(\"here\")\n",
    "    out = torch.nn.functional.grid_sample(\n",
    "        core,\n",
    "        grid = torch.nn.functional.tanh(model.readout.position.mean.expand(core.size(0), -1, -1)).unsqueeze(dim=2),\n",
    "        mode = \"bilinear\",\n",
    "        padding_mode=\"border\",\n",
    "        align_corners=False\n",
    "    )\n",
    "    return out\n",
    "\n",
    "#process_core_output(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers to use: 1\n",
      "hidden\n",
      "torch.Size([3, 256, 18, 32])\n",
      "3 256 18 32\n",
      "4\n",
      "hidden: 256 feature maps\n",
      "  spacedims [14, 28, 256]\n",
      "  Total units: 100352\n"
     ]
    }
   ],
   "source": [
    "############# COLLECT LAYERS #################\n",
    "\n",
    "# Function to get layers by name and type\n",
    "def get_layers_by_name_and_type(model, substrings, layer_types):\n",
    "    layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if any(sub in name for sub in substrings):\n",
    "            if isinstance(module, fnn.model.elements.Conv) and (\"skips\" not in name) and (\"convs.1\" not in name):\n",
    "                layers.append((name, module))\n",
    "    return layers\n",
    "\n",
    "# Collect layers based on the LAYER_TYPE and block\n",
    "if LAYER_TYPE == 'act':\n",
    "    layer_types = (nn.ReLU,)\n",
    "elif LAYER_TYPE == 'conv':\n",
    "    layer_types = (nn.Conv2d,)\n",
    "elif LAYER_TYPE == 'dense':\n",
    "    layer_types = (nn.Linear,)\n",
    "else:\n",
    "    raise ValueError('Invalid LAYER_TYPE')\n",
    "\n",
    "layer_types = 'Conv'\n",
    "\n",
    "layers_to_use = get_layers_by_name_and_type(model, block, layer_types)\n",
    "\n",
    "if 'hidden' in block:\n",
    "    layers_to_use.append((\"hidden\", model.core.recurrent))\n",
    "if 'interpolation' in block:\n",
    "    layers_to_use.append((\"interpolation\", model.readout.position))\n",
    "if 'readout' in block:\n",
    "    layers_to_use.append((\"readout\", model.readout))\n",
    "if 'perspective' in block:\n",
    "    layers_to_use = [(\"perspective\", model.perspective)]\n",
    "\n",
    "Nlayers = len(layers_to_use)\n",
    "print(f'Number of layers to use: {Nlayers}')\n",
    "\n",
    "\n",
    "# Set up hooks to capture activations\n",
    "activation_outputs = {}\n",
    "\n",
    "BATCH_SIZE=1\n",
    "counter = 0\n",
    "def get_activation(name):\n",
    "    print(name)\n",
    "\n",
    "    if name == \"hidden\":\n",
    "        def hook(model, input, output):\n",
    "            \n",
    "            if hasattr(module, 'past') and module.past is not None:\n",
    "                if isinstance(module.past, dict):\n",
    "                    \n",
    "                    if not name in activation_outputs:\n",
    "                        activation_outputs[name] = {}\n",
    "                    global counter\n",
    "                    activation_outputs[\"hidden\"][counter] = module.past[\"h\"].clone()\n",
    "                    #print(activation_outputs[\"hidden\"][counter].shape)\n",
    "                    #activation_outputs[\"hidden\"][counter] = activation_outputs[\"hidden\"][counter].clone()[:, :, :, 4:-4]\n",
    "                    #print(\"new\")\n",
    "                    #print(activation_outputs[\"hidden\"][counter].shape)\n",
    "                    global layers_to_use\n",
    "                    if name == layers_to_use[-1][0]:\n",
    "                        \n",
    "                        counter += 1\n",
    "                        counter = counter % 37\n",
    "    else:\n",
    "        def hook(model, input, output):\n",
    "            act_fct = torch.nn.GELU()\n",
    "            \n",
    "            if not name in activation_outputs:\n",
    "                activation_outputs[name] = {}\n",
    "            global counter\n",
    "            if name == \"core.recurrent.out\" and get_pos:\n",
    "                activation_outputs[name][counter] = process_core_output(output.detach())\n",
    "            else:\n",
    "                activation_outputs[name][counter] = output.detach()\n",
    "\n",
    "            if get_act:\n",
    "                activation_outputs[name][counter] = act_fct(activation_outputs[name][counter]) * 1.7015043497085571\n",
    "           \n",
    "            global layers_to_use\n",
    "            #print(f\"{counter}, {name}\")\n",
    "            if name == layers_to_use[-1][0]:\n",
    "                \n",
    "                counter += 1\n",
    "                counter = counter % 37\n",
    "\n",
    "    return hook\n",
    "\n",
    "for name, module in layers_to_use:\n",
    "    if name == \"hidden\":\n",
    "        \n",
    "        model.core.recurrent.register_forward_hook(get_activation(name))\n",
    "\n",
    "    else:\n",
    "        module.register_forward_hook(get_activation(name))\n",
    "\n",
    "frames = concatenate([\n",
    "    full(shape=[1, 2, 1, 144, 256], dtype=\"uint8\", fill_value=0),   # 1 second of black\n",
    "    full(shape=[1, 2, 1, 144, 256], dtype=\"uint8\", fill_value=128), # 1 second of gray\n",
    "    full(shape=[1, 2, 1, 144, 256], dtype=\"uint8\", fill_value=255), # 1 second of white\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    response = model(torch.Tensor(frames).to(device))\n",
    "\n",
    "\n",
    "# Collect output shapes and compute pads\n",
    "MAX_SIDE = 32\n",
    "all_layer_totfmaps = []\n",
    "all_layer_spacedims = []\n",
    "out_pads = []\n",
    "for name, module in layers_to_use:\n",
    "    output = activation_outputs[name][0]\n",
    "    #print(\"here\")\n",
    "    print(output.shape)\n",
    "    if \"interpolation\" in name or \"recurrent\" in name or 'readout' in name:\n",
    "        pad = False\n",
    "    else:\n",
    "        pad = True\n",
    "\n",
    "    if not pad:\n",
    "        output = output[:, :40]\n",
    "    batch_size, channels, height, width = output.shape\n",
    "    print(batch_size, channels, height, width)\n",
    "    totfmaps = channels\n",
    "\n",
    "    \n",
    "    if pad:\n",
    "        out_pad = height // 4 #max(1, (height - MAX_SIDE) // 2) * 3\n",
    "        print(out_pad)\n",
    "        h = height - out_pad\n",
    "        w = width - out_pad\n",
    "    else:\n",
    "        out_pad = 0\n",
    "        h = height\n",
    "        w = width\n",
    "        output = 0\n",
    "\n",
    "    spacedims = [h, w, totfmaps]\n",
    "    all_layer_totfmaps.append(totfmaps)\n",
    "    all_layer_spacedims.append(spacedims)\n",
    "    out_pads.append(out_pad)\n",
    "\n",
    "all_layer_nunits = [np.prod(lspcd) for lspcd in all_layer_spacedims]\n",
    "\n",
    "for li, (name, module) in enumerate(layers_to_use):\n",
    "    print(f'{name}: {all_layer_totfmaps[li]} feature maps')\n",
    "    print('  spacedims', all_layer_spacedims[li])\n",
    "    print('  Total units:', all_layer_nunits[li], flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot_stims 88\n",
      "frames_per_stim 37\n",
      "*INSTANCE 0 ...........\n",
      "*INSTANCE 1 ...........\n",
      "*INSTANCE 2 ...........\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x100 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "############# LOAD FLOW STIM FRAMES #################\n",
    "counter = 0\n",
    "orig_shape = (800, 600)\n",
    "\n",
    "mydirs = list(map(str, range(0, 360, 45)))\n",
    "categories = ['grat_W12', 'grat_W1', 'grat_W2',\n",
    "              'neg1dotflow_D1_bg', 'neg3dotflow_D1_bg', 'neg1dotflow_D2_bg', 'neg3dotflow_D2_bg',\n",
    "              'pos1dotflow_D1_bg', 'pos3dotflow_D1_bg', 'pos1dotflow_D2_bg', 'pos3dotflow_D2_bg']\n",
    "\n",
    "topdir = 'flowstims'\n",
    "NDIRS = len(mydirs)\n",
    "tot_stims = len(categories) * NDIRS\n",
    "print('tot_stims', tot_stims, flush=True)\n",
    "frames_per_stim = (trial_len // stride)\n",
    "print('frames_per_stim', frames_per_stim)\n",
    "\n",
    "# Create flow datasets (placeholder function)\n",
    "flow_datasets = createFlowDataset(categories, topdir, mydirs, orig_shape, input_shape,\n",
    "                                  scl_factor, N_INSTANCES, trial_len, stride)\n",
    "\n",
    "# Show example of sequence of frames generated for a stimulus trial\n",
    "n_frames_to_show = 4\n",
    "interval = 37\n",
    "\n",
    "f, axes = subps(1, n_frames_to_show, 1, 1)\n",
    "for i in range(n_frames_to_show):\n",
    "    ax = axes[i]\n",
    "    img = flow_datasets[0][i * interval].reshape(input_shape)\n",
    "    ax.imshow(img, vmin=0, vmax=255, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3256, 36864)\n",
      "36864\n",
      "11.0\n"
     ]
    }
   ],
   "source": [
    "print(flow_datasets[0].shape)\n",
    "print(144*256)\n",
    "print(3256 / 8 / 37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def reshape_flow_img(raveled_1chan_img):\\n    img = raveled_1chan_img.reshape((37, input_shape[0], input_shape[1]))\\n    #img = np.stack([img, img, img], axis=0)  # Convert to 3 channels\\n    img = img.astype(np.uint8)\\n    return img\\n\\n\\n\\n\\n# Collect output shapes and compute pads\\nMAX_SIDE = 16\\nall_layer_totfmaps = []\\nall_layer_spacedims = []\\nout_pads = []\\nfor name, module in layers_to_use:\\n    for seq_idx in range(int(len(flow_datasets[0])/37)):\\n        sequence = reshape_flow_img(flow_datasets[0][seq_idx*37:(seq_idx+1)*37])\\n\\n        with torch.no_grad():\\n            response = model.predict(stimuli=sequence)\\n        for i in range(37):\\n            output = activation_outputs[name][i]\\n            plt.imshow(output.cpu()[0, 0], cmap=\"Grays\")\\n            plt.show()'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def reshape_flow_img(raveled_1chan_img):\n",
    "    img = raveled_1chan_img.reshape((37, input_shape[0], input_shape[1]))\n",
    "    #img = np.stack([img, img, img], axis=0)  # Convert to 3 channels\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collect output shapes and compute pads\n",
    "MAX_SIDE = 16\n",
    "all_layer_totfmaps = []\n",
    "all_layer_spacedims = []\n",
    "out_pads = []\n",
    "for name, module in layers_to_use:\n",
    "    for seq_idx in range(int(len(flow_datasets[0])/37)):\n",
    "        sequence = reshape_flow_img(flow_datasets[0][seq_idx*37:(seq_idx+1)*37])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            response = model.predict(stimuli=sequence)\n",
    "        for i in range(37):\n",
    "            output = activation_outputs[name][i]\n",
    "            plt.imshow(output.cpu()[0, 0], cmap=\"Grays\")\n",
    "            plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot # of images: 88 * 37 = 3256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTANCE 0\n",
      "(0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s)  Tot time = 7.7\n",
      "INSTANCE 1\n",
      "(0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s)  Tot time = 7.7\n",
      "INSTANCE 2\n",
      "(0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s) (0.1s)  Tot time = 7.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####################### COMPUTE ################\n",
    "\n",
    "def reshape_flow_img(raveled_1chan_img):\n",
    "    img = raveled_1chan_img.reshape((1, 37, 1, input_shape[0], input_shape[1]))\n",
    "    #img = np.stack([img, img, img], axis=0)  # Convert to 3 channels\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "TOL = 0\n",
    "\n",
    "n_orig_imgs = tot_stims\n",
    "n_shifts = frames_per_stim\n",
    "n_shifted_imgs = n_orig_imgs * n_shifts\n",
    "\n",
    "\n",
    "print('tot # of images:', n_orig_imgs, '*', n_shifts, '=', n_shifted_imgs)\n",
    "\n",
    "\n",
    "layer_outputs = []\n",
    "instance_layer_outputs = []\n",
    "\n",
    "for li in range(len(layers_to_use)):\n",
    "    shape = [n_shifted_imgs] + all_layer_spacedims[li]\n",
    "    layer_outputs.append(np.zeros(shape, dtype='float32'))\n",
    "    shape_inst = np.append([N_INSTANCES], shape)\n",
    "    instance_layer_outputs.append(np.zeros(shape_inst, dtype='float32'))\n",
    "\n",
    "for insti in range(N_INSTANCES):\n",
    "    extX = flow_datasets[insti]\n",
    "    assert extX.shape[0] == n_shifted_imgs\n",
    "\n",
    "    print('INSTANCE', insti)\n",
    "    start0 = time()\n",
    "    layer_output = []\n",
    "    for li in range(len(layers_to_use)):\n",
    "        layer_output.append([])\n",
    "\n",
    "    for seq_idx in range(int(len(extX)/37)):\n",
    "        start = time()\n",
    "        #print(bb, end=' ', flush=True)\n",
    "\n",
    "        # Prepare batch\n",
    "        sequence = extX[seq_idx*37:(seq_idx+1)*37]\n",
    "        sequence = reshape_flow_img(extX[seq_idx*37:(seq_idx+1)*37])\n",
    "\n",
    "\n",
    "        # Collect outputs per layer\n",
    "        activation_outputs.clear()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            _ = model(torch.Tensor(sequence).to(device))\n",
    "\n",
    "        # Collect outputs per layer\n",
    "        for li, (name, module) in enumerate(layers_to_use):\n",
    "            for t in range(trial_len):\n",
    "                output = activation_outputs[name][t].detach().cpu().numpy()\n",
    "                if not pad:\n",
    "                    output = output[:, :40]\n",
    "                #print(output.shape)\n",
    "                #print(all_layer_spacedims[li])\n",
    "                # Crop the output if needed\n",
    "                h, w, c = all_layer_spacedims[li]\n",
    "                out_pad = out_pads[li]\n",
    "                #print(out_pad)\n",
    "                if output.ndim == 4:\n",
    "                    # output shape: (batch_size, channels, height, width)\n",
    "                    output_cropped = output[:, :, out_pad: out_pad + h, out_pad: out_pad + w]\n",
    "                    # Rearrange to (batch_size, height, width, channels)\n",
    "                    output_cropped = np.transpose(output_cropped, (0, 2, 3, 1))\n",
    "                else:\n",
    "                    output_cropped = output  # For dense layers\n",
    "\n",
    "                #print(output_cropped.shape)\n",
    "\n",
    "                layer_output[li].append(output_cropped)\n",
    "\n",
    "        print('(%.1fs) ' % (time() - start), end='', flush=True)\n",
    "    print(' Tot time = %.1f' % (time() - start0), flush=True)\n",
    "\n",
    "    # After processing all batches for this instance, concatenate outputs\n",
    "    for li in range(len(layers_to_use)):\n",
    "        #print(li)\n",
    "        #print([l.shape for l in layer_output[li]])\n",
    "        layer_output[li] = np.concatenate(layer_output[li], axis=0)\n",
    "        layer_outputs[li] += layer_output[li]\n",
    "        instance_layer_outputs[li][insti] = layer_output[li]\n",
    "\n",
    "\n",
    "# Average over instances\n",
    "for li in range(len(layers_to_use)):\n",
    "    layer_outputs[li] /= N_INSTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if layers_to_use[-1][0] == \"hidden\" and save_hidden:\n",
    "    np.save(\"../data/hidden_states.npy\", layer_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activities per img: 0"
     ]
    }
   ],
   "source": [
    "################### SUMMARIZE ACTIVITY ###########\n",
    "\n",
    "print('Activities per img:', end=' ')\n",
    "all_neurons_maxs = []\n",
    "all_neurons_means = []\n",
    "all_per_img_output = []\n",
    "for li in range(len(layers_to_use)):\n",
    "    print(li, end='', flush=True)\n",
    "    layer_output_ = layer_outputs[li].copy()\n",
    "\n",
    "\n",
    "    layer_output_[layer_output_ < 0] = 0\n",
    "\n",
    "    nfmaps = layer_output_.shape[3]\n",
    "    # Reshape to [n_orig_imgs, n_shifts, nfmaps, -1]\n",
    "    orig_per_img_output = np.moveaxis(layer_output_, -1, 1).reshape([n_orig_imgs, n_shifts, nfmaps, -1])\n",
    "    orig_per_img_output = np.moveaxis(orig_per_img_output, 1, -1)\n",
    "\n",
    "\n",
    "\n",
    "    # Normalize each image by the max\n",
    "    layer_output_ /= np.maximum(layer_output_.max((1, 2, 3), keepdims=True), 1e-8)\n",
    "\n",
    "    per_img_output = np.moveaxis(layer_output_, -1, 1).reshape([n_orig_imgs, n_shifts, nfmaps, -1])\n",
    "    per_img_output = np.moveaxis(per_img_output, 1, -1)\n",
    "\n",
    "    tot_n_neurons = np.prod(layer_output_.shape[1:])\n",
    "\n",
    "    neurons_maxs = np.zeros(per_img_output.shape[1:3])\n",
    "    neurons_means = np.zeros(per_img_output.shape[1:3])\n",
    "\n",
    "    for imi in range(n_orig_imgs):\n",
    "\n",
    "        if samp_max_one_dir and imi % NDIRS > 0:\n",
    "            im_avgs = 0\n",
    "        else:\n",
    "            im_avgs = per_img_output[imi].mean(2)  # Averaging across time\n",
    "        neurons_maxs = np.maximum(neurons_maxs, im_avgs)\n",
    "        neurons_means += im_avgs\n",
    "        \n",
    "    neurons_means /= n_orig_imgs\n",
    "\n",
    "    idxs = neurons_maxs.mean(1).argsort()\n",
    "\n",
    "    if li == 0:\n",
    "        all_neurons_maxs = neurons_maxs\n",
    "        all_neurons_means = neurons_means\n",
    "        all_per_img_output = orig_per_img_output\n",
    "    else:\n",
    "        all_neurons_maxs = np.concatenate([all_neurons_maxs, neurons_maxs], 0)\n",
    "        all_neurons_means = np.concatenate([all_neurons_means, neurons_means], 0)\n",
    "        all_per_img_output = np.concatenate([all_per_img_output, orig_per_img_output], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "50\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "############# SAMPLE NEURONS ###########\n",
    "\n",
    "nfmaps, n_neurons_per_fmap = all_neurons_maxs.shape\n",
    "layer_is_per_fmap = np.concatenate([li * np.ones(nf) for li, nf in enumerate(all_layer_totfmaps)])\n",
    "np.random.seed(seed)\n",
    "\n",
    "maxsmean = all_neurons_maxs.mean(1)\n",
    "nonzero_indices = (~np.isclose(maxsmean, 0)).sum()\n",
    "n_fmaps_to_sample_ = min(n_fmaps_to_sample, nonzero_indices)\n",
    "print(n_fmaps_to_sample)\n",
    "if fmap_samp_method == 'maxFr':\n",
    "    probabilities = maxsmean / maxsmean.sum()\n",
    "    top_fmaps = np.random.choice(range(nfmaps), n_fmaps_to_sample_, replace=False, p=probabilities)\n",
    "elif fmap_samp_method == \"random\":\n",
    "    top_fmaps = np.random.choice(range(nfmaps), n_fmaps_to_sample_, replace=False) \n",
    "else:\n",
    "    raise ValueError('Invalid fmap_samp_method')\n",
    "\n",
    "# Pick active neurons in each of these feature maps\n",
    "sampled_neurons = []\n",
    "\n",
    "samples_per_fmap = min(samples_per_fmap, all_neurons_means.shape[1])\n",
    "print(samples_per_fmap)\n",
    "for fi in top_fmaps:\n",
    "    if neur_samp_method == 'maxNr':\n",
    "        neuron_vals = all_neurons_maxs[fi]\n",
    "        nonzero_neurons = (~np.isclose(neuron_vals, 0)).sum()\n",
    "        samples_per_fmap_ = min(samples_per_fmap, nonzero_neurons)\n",
    "        probabilities = neuron_vals / neuron_vals.sum()\n",
    "        top_nis = np.random.choice(range(n_neurons_per_fmap), samples_per_fmap_, replace=False, p=probabilities)\n",
    "    else:\n",
    "        raise ValueError('Invalid neur_samp_method')\n",
    "    sampled_neurons += list(fi * n_neurons_per_fmap + top_nis)\n",
    "sampled_neurons = np.array(sampled_neurons)\n",
    "n_neurons_to_pick = len(sampled_neurons)\n",
    "print(n_neurons_to_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######### BUILD TENSOR ##########\n",
    "\n",
    "def get_neuron_pos(ni):\n",
    "    \"\"\"From sampled indices ni, get original indices back (layer index, fmap, posi, posj, raveled_idx)\"\"\"\n",
    "    fi = ni // n_neurons_per_fmap\n",
    "    li = int(layer_is_per_fmap[fi])\n",
    "    ij = ni % n_neurons_per_fmap\n",
    "    h, w, _ = all_layer_spacedims[li]\n",
    "    ii = ij // w\n",
    "    jj = ij % w\n",
    "    return li, fi, ii, jj, ij\n",
    "\n",
    "assert n_orig_imgs // NDIRS == len(categories)\n",
    "\n",
    "tensorX = np.zeros((n_neurons_to_pick, len(categories), NDIRS, n_shifts))\n",
    "neurons_used = np.empty((n_neurons_to_pick, 5), dtype='int')\n",
    "\n",
    "# Collect PSTs for those sampled neurons\n",
    "for nii, ni in enumerate(sampled_neurons):\n",
    "    li, fi, ii, jj, posi = get_neuron_pos(ni)\n",
    "    neurons_used[nii] = [li, fi, ii, jj, posi]\n",
    "\n",
    "    for cati in range(len(categories)):\n",
    "        pst = all_per_img_output[cati * NDIRS: (cati + 1) * NDIRS, fi, posi, :]\n",
    "        tensorX[nii, cati] = pst\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto1_act_i3_n2000_SCL0_7_TL37_hidden_maxFr_maxNr_seed1\n",
      "tensor4d_auto1_act_i3_n2000_SCL0_7_TL37_hidden_maxFr_maxNr_seed1.npy Saved.\n",
      "neurons_used_auto1_act_i3_n2000_SCL0_7_TL37_hidden_maxFr_maxNr_seed1.npy Saved.\n"
     ]
    }
   ],
   "source": [
    "if get_pos:\n",
    "    block[block == 'recurrent.out'] = 'position'\n",
    "for i in range(len(block)):\n",
    "    block[i] = block[i].replace('.', '')\n",
    "SUFFIX = f\"{model_name}_{LAYER_TYPE}_i{N_INSTANCES}_n{n_neurons_to_pick}_SCL{str(scl_factor).replace('.', '_')}_TL{trial_len}_{'_'.join(block)}_{fmap_samp_method}_{neur_samp_method}\"\n",
    "if samp_max_one_dir:\n",
    "    SUFFIX += '_onedir'\n",
    "if seed > 0:\n",
    "    SUFFIX += f'_seed{seed}'\n",
    "\n",
    "print(SUFFIX)\n",
    "\n",
    "directory = 'sampled_data'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "if os.path.exists(f'sampled_data/tensor4d_{SUFFIX}.npy'):\n",
    "    print(\"Files already exist, please delete them to prevent conflicts.\")\n",
    "else:\n",
    "    \n",
    "    np.save(f'sampled_data/tensor4d_{SUFFIX}.npy', tensorX)\n",
    "    print(f'tensor4d_{SUFFIX}.npy Saved.')\n",
    "\n",
    "    np.save(f'sampled_data/neurons_used_{SUFFIX}.npy', neurons_used)\n",
    "    print(f'neurons_used_{SUFFIX}.npy Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
