{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from different FNN layers\n",
    "\n",
    "Sampling from the output is not yet integrated here, use sampling-fnn-outputs.ipynb for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import createFlowDataset, subps  # Assuming these functions are in your utils module\n",
    "from glob import glob\n",
    "from time import time\n",
    "import sys\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "print(torch.__version__)  # E.g., '1.10.0'\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "import fnn\n",
    "from fnn import microns\n",
    "from numpy import full, concatenate\n",
    "from fnn.microns.build import frame_autoregressive_model\n",
    "\n",
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Finds the most recent checkpoint file based on epoch number.\"\"\"\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"autoregressive_fnn_epoch_\") and f.endswith(\".pt\")]\n",
    "    if not checkpoints:\n",
    "        raise FileNotFoundError(f\"No checkpoint files found in {checkpoint_dir}\")\n",
    "    \n",
    "    # Extract epoch number and sort\n",
    "    epochs = [int(f.split('_')[-1].replace('.pt', '')) for f in checkpoints]\n",
    "    latest_epoch = max(epochs)\n",
    "    latest_epoch = 8\n",
    "    latest_file = os.path.join(checkpoint_dir, f\"autoregressive_fnn_epoch_{latest_epoch}.pt\")\n",
    "    return latest_file, latest_epoch\n",
    "\n",
    "checkpoint_dir = \"example_checkpoints\"\n",
    "pred_steps = 5\n",
    "latest_checkpoint_path, latest_epoch = get_latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "model = frame_autoregressive_model(pred_steps=pred_steps).to(device)\n",
    "model.load_state_dict(torch.load(latest_checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "################# SET PARAMS ##########################\n",
    "block = ['blocks.2'] # choose from 'inputs.0', 'inputs.1', 'inputs.2', 'blocks.0', 'blocks.1', 'blocks.2', 'hidden', 'recurrent.out', 'position', 'readout'\n",
    "n_fmaps_to_sample = 40\n",
    "samples_per_fmap = 50\n",
    "seed = 1\n",
    "\n",
    "################## MORE PARAMS ########################\n",
    "# I suggest leaving these unchanged for comparability\n",
    "\n",
    "LAYER_TYPE = 'act'\n",
    "MAX_SIDE = 32\n",
    "\n",
    "# Flow stimuli parameters\n",
    "scl_factor = 0.7\n",
    "N_INSTANCES = 3\n",
    "trial_len = 75 // 2  # Number of frames\n",
    "stride = 1\n",
    "\n",
    "model_name = 'auto'\n",
    "\n",
    "## SAMPLING\n",
    "fmap_samp_method = 'maxFr'\n",
    "samp_max_one_dir = False # samples high activities to horizontal movement to the right only if set to True\n",
    "neur_samp_method = 'maxNr'\n",
    "\n",
    "input_shape = (144, 256)\n",
    "save_hidden = False # hacky hidden state debugging\n",
    "\n",
    "get_pos = True if \"position\" in block else False\n",
    "if get_pos:\n",
    "    block[block == \"position\"] = 'recurrent.out'\n",
    "\n",
    "get_act = any(item in block for item in ['inputs.0', 'inputs.1', 'inputs.2', 'blocks.0', 'blocks.1', 'blocks.2'])\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(f\"Module name: {name}, type: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_sample(batch_size=1):\n",
    "\n",
    "    units = 9941\n",
    "\n",
    "    mu = Parameter(torch.zeros(units, 2))\n",
    "    mu.scale = units\n",
    "    mu.decay = False\n",
    "\n",
    "    sigma = Parameter(torch.eye(2).repeat(units, 1, 1))\n",
    "    sigma.scale = units\n",
    "    sigma.decay = False\n",
    "\n",
    "\n",
    "    x = mu.repeat(batch_size, 1, 1)\n",
    "    x = x + torch.einsum(\"U C D , N U D -> N U C\", sigma, torch.randn_like(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "def process_core_output(core):\n",
    "    #position = torch.load(\"test_pos.pt\")\n",
    "    #self.position.mean.expand(core.size(0), -1, -1)\n",
    "    #print(\"here\")\n",
    "    out = torch.nn.functional.grid_sample(\n",
    "        core,\n",
    "        grid = torch.nn.functional.tanh(model.readout.position.mean.expand(core.size(0), -1, -1)).unsqueeze(dim=2),\n",
    "        mode = \"bilinear\",\n",
    "        padding_mode=\"border\",\n",
    "        align_corners=False\n",
    "    )\n",
    "    return out\n",
    "\n",
    "#process_core_output(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# COLLECT LAYERS #################\n",
    "\n",
    "# Function to get layers by name and type\n",
    "def get_layers_by_name_and_type(model, substrings, layer_types):\n",
    "    layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if any(sub in name for sub in substrings):\n",
    "            if isinstance(module, fnn.model.elements.Conv) and (\"skips\" not in name) and (\"convs.1\" not in name):\n",
    "                layers.append((name, module))\n",
    "    return layers\n",
    "\n",
    "# Collect layers based on the LAYER_TYPE and block\n",
    "if LAYER_TYPE == 'act':\n",
    "    layer_types = (nn.ReLU,)\n",
    "elif LAYER_TYPE == 'conv':\n",
    "    layer_types = (nn.Conv2d,)\n",
    "elif LAYER_TYPE == 'dense':\n",
    "    layer_types = (nn.Linear,)\n",
    "else:\n",
    "    raise ValueError('Invalid LAYER_TYPE')\n",
    "\n",
    "layer_types = 'Conv'\n",
    "\n",
    "layers_to_use = get_layers_by_name_and_type(model, block, layer_types)\n",
    "\n",
    "if 'hidden' in block:\n",
    "    layers_to_use.append((\"hidden\", model.core.recurrent))\n",
    "if 'interpolation' in block:\n",
    "    layers_to_use.append((\"interpolation\", model.readout.position))\n",
    "if 'readout' in block:\n",
    "    layers_to_use.append((\"readout\", model.readout))\n",
    "if 'perspective' in block:\n",
    "    layers_to_use = [(\"perspective\", model.perspective)]\n",
    "\n",
    "Nlayers = len(layers_to_use)\n",
    "print(f'Number of layers to use: {Nlayers}')\n",
    "\n",
    "\n",
    "# Set up hooks to capture activations\n",
    "activation_outputs = {}\n",
    "\n",
    "BATCH_SIZE=1\n",
    "counter = 0\n",
    "def get_activation(name):\n",
    "    print(name)\n",
    "\n",
    "    if name == \"hidden\":\n",
    "        def hook(model, input, output):\n",
    "            \n",
    "            if hasattr(module, 'past') and module.past is not None:\n",
    "                if isinstance(module.past, dict):\n",
    "                    \n",
    "                    if not name in activation_outputs:\n",
    "                        activation_outputs[name] = {}\n",
    "                    global counter\n",
    "                    activation_outputs[\"hidden\"][counter] = module.past[\"h\"].clone()\n",
    "                    #print(activation_outputs[\"hidden\"][counter].shape)\n",
    "                    #activation_outputs[\"hidden\"][counter] = activation_outputs[\"hidden\"][counter].clone()[:, :, :, 4:-4]\n",
    "                    #print(\"new\")\n",
    "                    #print(activation_outputs[\"hidden\"][counter].shape)\n",
    "                    global layers_to_use\n",
    "                    if name == layers_to_use[-1][0]:\n",
    "                        \n",
    "                        counter += 1\n",
    "                        counter = counter % 37\n",
    "    else:\n",
    "        def hook(model, input, output):\n",
    "            act_fct = torch.nn.GELU()\n",
    "            \n",
    "            if not name in activation_outputs:\n",
    "                activation_outputs[name] = {}\n",
    "            global counter\n",
    "            if name == \"core.recurrent.out\" and get_pos:\n",
    "                activation_outputs[name][counter] = process_core_output(output.detach())\n",
    "            else:\n",
    "                activation_outputs[name][counter] = output.detach()\n",
    "\n",
    "            if get_act:\n",
    "                activation_outputs[name][counter] = act_fct(activation_outputs[name][counter]) * 1.7015043497085571\n",
    "           \n",
    "            global layers_to_use\n",
    "            #print(f\"{counter}, {name}\")\n",
    "            if name == layers_to_use[-1][0]:\n",
    "                \n",
    "                counter += 1\n",
    "                counter = counter % 37\n",
    "\n",
    "    return hook\n",
    "\n",
    "for name, module in layers_to_use:\n",
    "    if name == \"hidden\":\n",
    "        \n",
    "        model.core.recurrent.register_forward_hook(get_activation(name))\n",
    "\n",
    "    else:\n",
    "        module.register_forward_hook(get_activation(name))\n",
    "\n",
    "frames = concatenate([\n",
    "    full(shape=[1, 2, 1, 144, 256], dtype=\"uint8\", fill_value=0),   # 1 second of black\n",
    "    full(shape=[1, 2, 1, 144, 256], dtype=\"uint8\", fill_value=128), # 1 second of gray\n",
    "    full(shape=[1, 2, 1, 144, 256], dtype=\"uint8\", fill_value=255), # 1 second of white\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    response = model(torch.Tensor(frames).to(device))\n",
    "\n",
    "\n",
    "# Collect output shapes and compute pads\n",
    "MAX_SIDE = 32\n",
    "all_layer_totfmaps = []\n",
    "all_layer_spacedims = []\n",
    "out_pads = []\n",
    "for name, module in layers_to_use:\n",
    "    output = activation_outputs[name][0]\n",
    "    #print(\"here\")\n",
    "    print(output.shape)\n",
    "    if \"interpolation\" in name or \"recurrent\" in name or 'readout' in name:\n",
    "        pad = False\n",
    "    else:\n",
    "        pad = True\n",
    "\n",
    "    if not pad:\n",
    "        output = output[:, :40]\n",
    "    batch_size, channels, height, width = output.shape\n",
    "    print(batch_size, channels, height, width)\n",
    "    totfmaps = channels\n",
    "\n",
    "    \n",
    "    if pad:\n",
    "        out_pad = height // 4 #max(1, (height - MAX_SIDE) // 2) * 3\n",
    "        print(out_pad)\n",
    "        h = height - out_pad\n",
    "        w = width - out_pad\n",
    "    else:\n",
    "        out_pad = 0\n",
    "        h = height\n",
    "        w = width\n",
    "        output = 0\n",
    "\n",
    "    spacedims = [h, w, totfmaps]\n",
    "    all_layer_totfmaps.append(totfmaps)\n",
    "    all_layer_spacedims.append(spacedims)\n",
    "    out_pads.append(out_pad)\n",
    "\n",
    "all_layer_nunits = [np.prod(lspcd) for lspcd in all_layer_spacedims]\n",
    "\n",
    "for li, (name, module) in enumerate(layers_to_use):\n",
    "    print(f'{name}: {all_layer_totfmaps[li]} feature maps')\n",
    "    print('  spacedims', all_layer_spacedims[li])\n",
    "    print('  Total units:', all_layer_nunits[li], flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############# LOAD FLOW STIM FRAMES #################\n",
    "counter = 0\n",
    "orig_shape = (800, 600)\n",
    "\n",
    "mydirs = list(map(str, range(0, 360, 45)))\n",
    "categories = ['grat_W12', 'grat_W1', 'grat_W2',\n",
    "              'neg1dotflow_D1_bg', 'neg3dotflow_D1_bg', 'neg1dotflow_D2_bg', 'neg3dotflow_D2_bg',\n",
    "              'pos1dotflow_D1_bg', 'pos3dotflow_D1_bg', 'pos1dotflow_D2_bg', 'pos3dotflow_D2_bg']\n",
    "\n",
    "topdir = 'flowstims'\n",
    "NDIRS = len(mydirs)\n",
    "tot_stims = len(categories) * NDIRS\n",
    "print('tot_stims', tot_stims, flush=True)\n",
    "frames_per_stim = (trial_len // stride)\n",
    "print('frames_per_stim', frames_per_stim)\n",
    "\n",
    "# Create flow datasets (placeholder function)\n",
    "flow_datasets = createFlowDataset(categories, topdir, mydirs, orig_shape, input_shape,\n",
    "                                  scl_factor, N_INSTANCES, trial_len, stride)\n",
    "\n",
    "# Show example of sequence of frames generated for a stimulus trial\n",
    "n_frames_to_show = 4\n",
    "interval = 37\n",
    "\n",
    "f, axes = subps(1, n_frames_to_show, 1, 1)\n",
    "for i in range(n_frames_to_show):\n",
    "    ax = axes[i]\n",
    "    img = flow_datasets[0][i * interval].reshape(input_shape)\n",
    "    ax.imshow(img, vmin=0, vmax=255, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flow_datasets[0].shape)\n",
    "print(144*256)\n",
    "print(3256 / 8 / 37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def reshape_flow_img(raveled_1chan_img):\n",
    "    img = raveled_1chan_img.reshape((37, input_shape[0], input_shape[1]))\n",
    "    #img = np.stack([img, img, img], axis=0)  # Convert to 3 channels\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collect output shapes and compute pads\n",
    "MAX_SIDE = 16\n",
    "all_layer_totfmaps = []\n",
    "all_layer_spacedims = []\n",
    "out_pads = []\n",
    "for name, module in layers_to_use:\n",
    "    for seq_idx in range(int(len(flow_datasets[0])/37)):\n",
    "        sequence = reshape_flow_img(flow_datasets[0][seq_idx*37:(seq_idx+1)*37])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            response = model.predict(stimuli=sequence)\n",
    "        for i in range(37):\n",
    "            output = activation_outputs[name][i]\n",
    "            plt.imshow(output.cpu()[0, 0], cmap=\"Grays\")\n",
    "            plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################### COMPUTE ################\n",
    "\n",
    "def reshape_flow_img(raveled_1chan_img):\n",
    "    img = raveled_1chan_img.reshape((1, 37, 1, input_shape[0], input_shape[1]))\n",
    "    #img = np.stack([img, img, img], axis=0)  # Convert to 3 channels\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "TOL = 0\n",
    "\n",
    "n_orig_imgs = tot_stims\n",
    "n_shifts = frames_per_stim\n",
    "n_shifted_imgs = n_orig_imgs * n_shifts\n",
    "\n",
    "\n",
    "print('tot # of images:', n_orig_imgs, '*', n_shifts, '=', n_shifted_imgs)\n",
    "\n",
    "\n",
    "layer_outputs = []\n",
    "instance_layer_outputs = []\n",
    "\n",
    "for li in range(len(layers_to_use)):\n",
    "    shape = [n_shifted_imgs] + all_layer_spacedims[li]\n",
    "    layer_outputs.append(np.zeros(shape, dtype='float32'))\n",
    "    shape_inst = np.append([N_INSTANCES], shape)\n",
    "    instance_layer_outputs.append(np.zeros(shape_inst, dtype='float32'))\n",
    "\n",
    "for insti in range(N_INSTANCES):\n",
    "    extX = flow_datasets[insti]\n",
    "    assert extX.shape[0] == n_shifted_imgs\n",
    "\n",
    "    print('INSTANCE', insti)\n",
    "    start0 = time()\n",
    "    layer_output = []\n",
    "    for li in range(len(layers_to_use)):\n",
    "        layer_output.append([])\n",
    "\n",
    "    for seq_idx in range(int(len(extX)/37)):\n",
    "        start = time()\n",
    "        #print(bb, end=' ', flush=True)\n",
    "\n",
    "        # Prepare batch\n",
    "        sequence = extX[seq_idx*37:(seq_idx+1)*37]\n",
    "        sequence = reshape_flow_img(extX[seq_idx*37:(seq_idx+1)*37])\n",
    "\n",
    "\n",
    "        # Collect outputs per layer\n",
    "        activation_outputs.clear()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            _ = model(torch.Tensor(sequence).to(device))\n",
    "\n",
    "        # Collect outputs per layer\n",
    "        for li, (name, module) in enumerate(layers_to_use):\n",
    "            for t in range(trial_len):\n",
    "                output = activation_outputs[name][t].detach().cpu().numpy()\n",
    "                if not pad:\n",
    "                    output = output[:, :40]\n",
    "                #print(output.shape)\n",
    "                #print(all_layer_spacedims[li])\n",
    "                # Crop the output if needed\n",
    "                h, w, c = all_layer_spacedims[li]\n",
    "                out_pad = out_pads[li]\n",
    "                #print(out_pad)\n",
    "                if output.ndim == 4:\n",
    "                    # output shape: (batch_size, channels, height, width)\n",
    "                    output_cropped = output[:, :, out_pad: out_pad + h, out_pad: out_pad + w]\n",
    "                    # Rearrange to (batch_size, height, width, channels)\n",
    "                    output_cropped = np.transpose(output_cropped, (0, 2, 3, 1))\n",
    "                else:\n",
    "                    output_cropped = output  # For dense layers\n",
    "\n",
    "                #print(output_cropped.shape)\n",
    "\n",
    "                layer_output[li].append(output_cropped)\n",
    "\n",
    "        print('(%.1fs) ' % (time() - start), end='', flush=True)\n",
    "    print(' Tot time = %.1f' % (time() - start0), flush=True)\n",
    "\n",
    "    # After processing all batches for this instance, concatenate outputs\n",
    "    for li in range(len(layers_to_use)):\n",
    "        #print(li)\n",
    "        #print([l.shape for l in layer_output[li]])\n",
    "        layer_output[li] = np.concatenate(layer_output[li], axis=0)\n",
    "        layer_outputs[li] += layer_output[li]\n",
    "        instance_layer_outputs[li][insti] = layer_output[li]\n",
    "\n",
    "\n",
    "# Average over instances\n",
    "for li in range(len(layers_to_use)):\n",
    "    layer_outputs[li] /= N_INSTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if layers_to_use[-1][0] == \"hidden\" and save_hidden:\n",
    "    np.save(\"../data/hidden_states.npy\", layer_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### SUMMARIZE ACTIVITY ###########\n",
    "\n",
    "print('Activities per img:', end=' ')\n",
    "all_neurons_maxs = []\n",
    "all_neurons_means = []\n",
    "all_per_img_output = []\n",
    "for li in range(len(layers_to_use)):\n",
    "    print(li, end='', flush=True)\n",
    "    layer_output_ = layer_outputs[li].copy()\n",
    "\n",
    "\n",
    "    layer_output_[layer_output_ < 0] = 0\n",
    "\n",
    "    nfmaps = layer_output_.shape[3]\n",
    "    # Reshape to [n_orig_imgs, n_shifts, nfmaps, -1]\n",
    "    orig_per_img_output = np.moveaxis(layer_output_, -1, 1).reshape([n_orig_imgs, n_shifts, nfmaps, -1])\n",
    "    orig_per_img_output = np.moveaxis(orig_per_img_output, 1, -1)\n",
    "\n",
    "\n",
    "\n",
    "    # Normalize each image by the max\n",
    "    layer_output_ /= np.maximum(layer_output_.max((1, 2, 3), keepdims=True), 1e-8)\n",
    "\n",
    "    per_img_output = np.moveaxis(layer_output_, -1, 1).reshape([n_orig_imgs, n_shifts, nfmaps, -1])\n",
    "    per_img_output = np.moveaxis(per_img_output, 1, -1)\n",
    "\n",
    "    tot_n_neurons = np.prod(layer_output_.shape[1:])\n",
    "\n",
    "    neurons_maxs = np.zeros(per_img_output.shape[1:3])\n",
    "    neurons_means = np.zeros(per_img_output.shape[1:3])\n",
    "\n",
    "    for imi in range(n_orig_imgs):\n",
    "\n",
    "        if samp_max_one_dir and imi % NDIRS > 0:\n",
    "            im_avgs = 0\n",
    "        else:\n",
    "            im_avgs = per_img_output[imi].mean(2)  # Averaging across time\n",
    "        neurons_maxs = np.maximum(neurons_maxs, im_avgs)\n",
    "        neurons_means += im_avgs\n",
    "        \n",
    "    neurons_means /= n_orig_imgs\n",
    "\n",
    "    idxs = neurons_maxs.mean(1).argsort()\n",
    "\n",
    "    if li == 0:\n",
    "        all_neurons_maxs = neurons_maxs\n",
    "        all_neurons_means = neurons_means\n",
    "        all_per_img_output = orig_per_img_output\n",
    "    else:\n",
    "        all_neurons_maxs = np.concatenate([all_neurons_maxs, neurons_maxs], 0)\n",
    "        all_neurons_means = np.concatenate([all_neurons_means, neurons_means], 0)\n",
    "        all_per_img_output = np.concatenate([all_per_img_output, orig_per_img_output], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# SAMPLE NEURONS ###########\n",
    "\n",
    "nfmaps, n_neurons_per_fmap = all_neurons_maxs.shape\n",
    "layer_is_per_fmap = np.concatenate([li * np.ones(nf) for li, nf in enumerate(all_layer_totfmaps)])\n",
    "np.random.seed(seed)\n",
    "\n",
    "maxsmean = all_neurons_maxs.mean(1)\n",
    "nonzero_indices = (~np.isclose(maxsmean, 0)).sum()\n",
    "n_fmaps_to_sample_ = min(n_fmaps_to_sample, nonzero_indices)\n",
    "print(n_fmaps_to_sample)\n",
    "if fmap_samp_method == 'maxFr':\n",
    "    probabilities = maxsmean / maxsmean.sum()\n",
    "    top_fmaps = np.random.choice(range(nfmaps), n_fmaps_to_sample_, replace=False, p=probabilities)\n",
    "elif fmap_samp_method == \"random\":\n",
    "    top_fmaps = np.random.choice(range(nfmaps), n_fmaps_to_sample_, replace=False) \n",
    "else:\n",
    "    raise ValueError('Invalid fmap_samp_method')\n",
    "\n",
    "# Pick active neurons in each of these feature maps\n",
    "sampled_neurons = []\n",
    "\n",
    "samples_per_fmap = min(samples_per_fmap, all_neurons_means.shape[1])\n",
    "print(samples_per_fmap)\n",
    "for fi in top_fmaps:\n",
    "    if neur_samp_method == 'maxNr':\n",
    "        neuron_vals = all_neurons_maxs[fi]\n",
    "        nonzero_neurons = (~np.isclose(neuron_vals, 0)).sum()\n",
    "        samples_per_fmap_ = min(samples_per_fmap, nonzero_neurons)\n",
    "        probabilities = neuron_vals / neuron_vals.sum()\n",
    "        top_nis = np.random.choice(range(n_neurons_per_fmap), samples_per_fmap_, replace=False, p=probabilities)\n",
    "    else:\n",
    "        raise ValueError('Invalid neur_samp_method')\n",
    "    sampled_neurons += list(fi * n_neurons_per_fmap + top_nis)\n",
    "sampled_neurons = np.array(sampled_neurons)\n",
    "n_neurons_to_pick = len(sampled_neurons)\n",
    "print(n_neurons_to_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######### BUILD TENSOR ##########\n",
    "\n",
    "def get_neuron_pos(ni):\n",
    "    \"\"\"From sampled indices ni, get original indices back (layer index, fmap, posi, posj, raveled_idx)\"\"\"\n",
    "    fi = ni // n_neurons_per_fmap\n",
    "    li = int(layer_is_per_fmap[fi])\n",
    "    ij = ni % n_neurons_per_fmap\n",
    "    h, w, _ = all_layer_spacedims[li]\n",
    "    ii = ij // w\n",
    "    jj = ij % w\n",
    "    return li, fi, ii, jj, ij\n",
    "\n",
    "assert n_orig_imgs // NDIRS == len(categories)\n",
    "\n",
    "tensorX = np.zeros((n_neurons_to_pick, len(categories), NDIRS, n_shifts))\n",
    "neurons_used = np.empty((n_neurons_to_pick, 5), dtype='int')\n",
    "\n",
    "# Collect PSTs for those sampled neurons\n",
    "for nii, ni in enumerate(sampled_neurons):\n",
    "    li, fi, ii, jj, posi = get_neuron_pos(ni)\n",
    "    neurons_used[nii] = [li, fi, ii, jj, posi]\n",
    "\n",
    "    for cati in range(len(categories)):\n",
    "        pst = all_per_img_output[cati * NDIRS: (cati + 1) * NDIRS, fi, posi, :]\n",
    "        tensorX[nii, cati] = pst\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_pos:\n",
    "    block[block == 'recurrent.out'] = 'position'\n",
    "for i in range(len(block)):\n",
    "    block[i] = block[i].replace('.', '')\n",
    "SUFFIX = f\"{model_name}_{LAYER_TYPE}_i{N_INSTANCES}_n{n_neurons_to_pick}_SCL{str(scl_factor).replace('.', '_')}_TL{trial_len}_{'_'.join(block)}_{fmap_samp_method}_{neur_samp_method}\"\n",
    "if samp_max_one_dir:\n",
    "    SUFFIX += '_onedir'\n",
    "if seed > 0:\n",
    "    SUFFIX += f'_seed{seed}'\n",
    "\n",
    "print(SUFFIX)\n",
    "\n",
    "directory = 'sampled_data'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "if os.path.exists(f'sampled_data/tensor4d_{SUFFIX}.npy'):\n",
    "    print(\"Files already exist, please delete them to prevent conflicts.\")\n",
    "else:\n",
    "    \n",
    "    np.save(f'sampled_data/tensor4d_{SUFFIX}.npy', tensorX)\n",
    "    print(f'tensor4d_{SUFFIX}.npy Saved.')\n",
    "\n",
    "    np.save(f'sampled_data/neurons_used_{SUFFIX}.npy', neurons_used)\n",
    "    print(f'neurons_used_{SUFFIX}.npy Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
